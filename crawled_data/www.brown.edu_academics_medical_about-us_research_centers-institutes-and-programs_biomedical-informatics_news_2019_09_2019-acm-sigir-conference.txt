Skip to main content Skip to primary site menu Google Tag Manager 2019 ACM SIGIR Conference Dr. Carsten Eickhoff attended the ACM SIGIR Conference in Paris, France from July 2019. He gave a presentation titled, “Unsupervised Learning of Parsimonious General-Purpose Embeddings for User and Location Modeling”, and presented a poster titled, “On the Effect of Low-Frequency Terms on Neural-IR Models”, which discusses recent findings in representation learning and neural-network based information management. Below are the abstracts for both presented works: Unsupervised Learning of Parsimonious General-Purpose Embeddings for User and Location Modeling (http://brown.edu/Research/AI/files/pubs/tois18.pdf) Many social network applications depend on robust representations of spatio-temporal data. In this work, we present an embedding model based on feed-forward neural networks which transforms social media checkins into dense feature vectors encoding geographic, temporal, and functional aspects for modeling places, neighborhoods, and users. In a range of experiments on real life data collected from Foursquare, we demonstrate our model’s effectiveness at characterizing places and people and its applicability in aforementioned problem domains. Finally, we select eight major cities around the globe and verify the robustness and generality of our model by transferring pre-trained models from one city to another, thereby alleviating the need for costly local training. On the Effect of Low-Frequency Terms on Neural-IR Models (http://brown.edu/Research/AI/files/pubs/sigir19.pdf) Low-frequency terms are a recurring challenge for information retrieval models, especially neural IR frameworks struggle with adequately capturing infrequently observed words. While these terms are often removed from neural models – mainly as a concession to efficiency demands – they traditionally play an important role in the performance of IR models. In this paper, we analyze the effects of low-frequency terms on the performance and robustness of neural IR models. We conduct controlled experiments on three recent neural IR models, trained on a large-scale passage retrieval collection. We evaluate the neural IR models with various vocabulary sizes for their respective word embeddings, considering different levels of constraints on the available GPU memory. We observe that despite the significant benefits of using larger vocabularies, the performance gap between the vocabularies can be, to a great extent, mitigated by extensive tuning of a related parameter: the number of documents to re-rank. Also in About Education Research News Collaborators Events People Explore Brown University Explore Brown University Learn More Brown Homepage About Brown Academics Admission Research Campus Life Find A to Z Index People Directory Information for Students Faculty Staff Families Alumni Friends & Neighbors Top Destinations Global Brown Watson Institute Brown & Providence Graduate School Alpert Medical School School of Public Health School of Engineering BCBI Community Mailing List Brown Center for Biomedical Informatics Brown Center for Biomedical Informatics Brown University Box G-R 233 Richmond Street Providence, RI 02912 Phone: 401-863-5938 Email [email protected] Twitter Twitter Giving to Brown Brown University Providence, Rhode Island 02912, USA Phone: 401-863-1000 Maps & Directions / Contact Us / Accessibility © 2022 Brown University Connect: [email protected] Facebook Twitter YouTube Instagram iTunes U LinkedIn SnapChat