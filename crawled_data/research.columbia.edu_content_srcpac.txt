Skip to main content Columbia University in the City of New York Toggle search Columbia University Columbia | Research Toggle search Submit keywords Search the site Main navigation expanded About Compliance Find Funding Offices Resources Safety Training The Office of the Executive Vice President for Research Announcements Calendar Directory News Office Locations Research Honors and Awards Additional Information Anti-Harassment Compliance in the News Conflict of Interest Economic Sanctions and Restricted Parties Export Controls International Research Policies and Handbooks Research Misconduct Research with Animals Research with Humans Responsible Conduct of Research Science & Security Office of Research Compliance and Training Report an Incident Additional Topics Find Funding using Pivot Proposal Submission Center Grants Limited Submissions RISE Internal Seed Funds Postdoc Funding Opportunities Executive Vice President for Research Clinical Trials Office Environmental Health & Safety Human Research Protection and IRBs Institute of Comparative Medicine Institutional Animal Care and Use Committee Postdoctoral Affairs Research Compliance and Training Research Initiatives and Development Sponsored Projects Administration Core & Shared Facilities Finder Directory Event Planning on Campus Forms Institutional Information Join a Study Online Research Systems Policies and Handbooks Postdoc Resources Rascal Research Data at Columbia Research Pharmacy Research and Data Integrity Program (ReaDI) Resources for Chairs Shared Research Computing Additional Resources Biological Controlled Substances Fire Hazardous Materials and Sustainability Laboratory and Research Occupational Radiation and Laser Safety Training Good Clinical Practice Human Subjects Protection IND/IDE Assistance Program Laboratory Animals Learn about Sponsored Projects PCRI Symposium Postdoc Training Rascal Research and Data Integrity Program (ReaDI) Responsible Conduct of Research Safety Trainings Sponsored Projects Certificate Program Training Finder Additional Training Columbia | Research You are here: Home Resources Shared Research Computing Policy Advisory Committee Shared Research Computing Policy Advisory Committee The Shared Research Computing Policy Advisory Committee (SRCPAC) is a faculty-dominated group focused on a variety of policy issues related to shared high performance computing for researchers on the Morningside and Manhattanville campuses. As an increasing number of disciplines engage in data-driven and/or computational methods in order to create and disseminate knowledge, there is a commensurate rise and duplication in the costs of individually establishing and maintaining the necessary resources. Sharing an independently maintained HPC simultaneously reduces expense while providing access to a larger machine than individual groups of researchers can typically deploy. SRCPAC is administratively supported by the Research Computing Services group within CU Information Technology, and the Office of Research Initiatives. The Committee additionally works in close partnership with Columbia University Libraries and the Data Science Institute. Please contact us at any time by emailing [email protected]. Expand all Collapse all Ginsburg HPC Cluster The Ginsburg High Performance Computing cluster, a $1.4 Million joint purchase by 33 research groups and departments, went live in February 2021 consisting of 139 nodes with a total of 4448 cores. Over a million-dollars expanded the cluster by 99 nodes with a total 3168 cores this Spring. The system now consists of 238 nodes with a total of 7616 cores (32 cores per node), including 22 GPU hardware accelerated systems allowing certain highly-parallelized applications to achieve performance levels far beyond what would be possible on conventional hardware. Ginsburg Cluster Specifications All servers are equipped with Dual Intel Xeon Gold 6226R processors (2.9 GHz): 157 Standard Nodes (192 GB) 48 High Memory Nodes (768 GB) 18 RTX 8000 GPU nodes (2 GPUs modules per server) 4 V100S GPU nodes (2 GPU modules per server) 6 A100 GPU Nodes (2 GPU modules per server) 5 A40 GPU Nodes (2 GPU modules per server) 779TB of DDN ES7790 Lustre storage EDR Infiniband Red Hat Enterprise Linux 8 Slurm job scheduler For more information, please visit Columbia's Shared High Performance Computing webpage. Price History by Cluster Ginsburg - Expansion (2021) Standard Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive. $ $7,850 High Memory Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 768 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive. $ $12,750 GPU Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and two Nvidia A40 48GB GPUs $ $13,750 GPU Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and one Nvidia A100 40GB GPU $ $13,000 GPU Node Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and two Nvidia A100 40GB GPUs $ $18,850 Storage (in Terabytes) (minimum of 1 per order) $ $350 Ginsburg - First Round (2020) Standard Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive. $ $7,850 High Memory Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 768 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive. $ $9,450 GPU Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and two Nvidia V100S 32GB GPUs. $ $17,500 GPU Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and two Nvidia Quadro RTX 8000 48GB GPUs. $ $13,750 GPU Node - Dual Xeon Gold 6226R processors (2.9 GHz, 16 cores each, 32 cores per server), 192 GB memory, 100 Gb/s Infiniband, 480 GB SSD drive, and two Nvidia Quadro RTX 6000 24GB GPUs. $ $12,050 Storage (in Terabytes) (minimum of 1 per order) $ $350 Terremoto - Expansion (2019) Standard Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 192 GB memory, EDR Infiniband, 480 GB SSD drive. $ $8,500 High Memory Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 768 GB memory, EDR Infiniband, 480 GB SSD drive. $ $15,200 GPU Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 192 GB memory, EDR Infiniband, 480 GB SSD drive, and one Nvidia 32GB V100 GPU. $ $17,000 GPU Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 192 GB memory, EDR Infiniband, 480 GB SSD drive, and two Nvidia 32GB V100 GPUs. $ $23,850 Storage (in Terabytes) (minimum of 1 per order) $ $250 Terremoto - First Round (2018) Standard Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 192 GB memory, EDR Infiniband, 480 GB SSD drive. $ $8,500 High Memory Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 768 GB memory, EDR Infiniband, 480 GB SSD drive. $ $15,000 GPU Node - Dual Skylake Gold 6126 processors (2.6 GHz, 12 cores each, 24 cores per server), 192 GB memory, EDR Infiniband, 480 GB SSD drive, and two Nvidia 16GB V100 GPUs. GPU servers may also be purchased in half increments of $11,000. $ $22,000 Storage (in Terabytes) (minimum of 1 per order) $ $275 Habanero - Expansion (2017) Standard Node - HP Enterprise XL170r with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 128 GB memory, and EDR Infiniband. $ $7,000 High Memory Node - HP Enterprise XL170r with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 512 GB memory, and EDR Infiniband. $ $16,000 GPU Node - HP Enterprise DL380 with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 128 GB memory, EDR Infiniband, and two Nvidia P100 GPUs. $ $17,000 Storage (in Terabytes) (minimum of 1 per order) $ $250 Habanero - First Round (2016) Standard Node - HP Enterprise XL170r with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 128 GB memory, and EDR Infiniband $ $5,630 High Memory Node - HP Enterprise XL170r with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 512 GB memory, and EDR Infiniband. $ $7,620 GPU Node - HP Enterprise DL380 with dual E5-2650v4 processors (2.2 GHz, 12 cores each,24 cores per server), 128 GB memory, EDR Infiniband, and four Nvidia K80 GPUs. $ $13,180 Storage (in Terabytes) (minimum of 1 per order) $ $250 Welcome from the Chair Dear Colleagues, I write to you enthusiastic over the Shared Research Computing Policy Advisory Committee’s (SRCPAC) advancement of the University’s high-performance computing resource. From its humble beginnings in 2011, research computing at Columbia is now something new under the sun. As SRCPAC’s Chair, I am tasked with representing faculty interests in comprehensive governance of the shared research computing facility (SRCF). Our community – including rotating subcommittees and working groups devoted to multiple strategic initiatives – is comprised of over 150 faculty, postdocs, staff, and students, and meets semiannually to review topics of considerable range, including cloud computing, educational workshops, facility operations, and policy changes. All Columbia faculty are invited and strongly encouraged to attend SRCPAC meetings; a faculty designee can attend in the event of scheduling conflicts. I hope that you will join us for the many discussions that the future holds. Formed in 2011, SRCPAC is the manifestation of a movement many years in the making. It is a unified effort in further developing the physical infrastructure, administrative network, and governance policies that are fundamental to innovative computational research and supporting corresponding grant-making activities. Columbia is a global leader in integrating data science methodologies across all domains and disciplines; this leadership is powerfully represented by our Data Science Institute, among many other academic units. The University is committed to furthering this integration and capitalizing upon new emergent opportunities in computationally-driven discovery. This is SRCPAC. In Fall 2016, SRCPAC achieved yet another seminal milestone: the installation of Habanero, Columbia's third high performance computing cluster for shared use among Columbia’s researchers. At an initial cost of $1.5 Million and expanded the following year, Habanero consists of 44 discrete group purchases and contains 302 compute notes and 800 terabytes of storage. Terremoto, Columbia's fourth HPC cluster to enter production, went live in December 2018 and was expanded in 2019. Terremoto is a joint purchase of 35 research groups and departments and consists of 137 compute nodes and over half a petabyte of storage. And most recently, the Ginsburg cluster, a joint purchase by 33 research groups and departments, was launched in February 2021. Ginsburg consists of 139 nodes including 22 GPU hardware accelerated systems. These significant strides were made possible in no small part by the tireless efforts and commitments of the Faculty of Arts and Sciences, The Fu Foundation School of Engineering & Applied Science, and CUIT, making Habanero and Terremoto collective achievements for which we should all be proud. The High Performance Operating Committee of users is chaired by my colleague, Dr. Kyle Mandli, Assistant Professor, Department of Applied Physics & Applied Mathematics. We are at a pivotal time in research – both generally and especially so at Columbia and I encourage you to explore the wealth of information found below regarding SRCPAC’s mission, structure, and emergent themes. Your inquiries and comments are welcome as we collectively decide how to navigate the future of research computing. As SRCPAC is a joint effort, there are two methods for communicating with staff resources: For technical questions related to HPC use or for general research computing support, please contact CUIT Research Computing Services at [email protected]; For policy, governance, and faculty affairs questions, please contact the Office of Research Initiatives at [email protected]. Thank you again for joining us in this exciting endeavor – we look forward to working with you. Chris Marianetti, PhD Chair, Shared Research Computing Policy Advisory Committee Associate Professor, Department of Applied Physics & Applied Mathematics SRCPAC Charter Excerpt from the SRCPAC Charter, November 9, 2011: "The Shared Research Computing Policy Advisory Committee (SRCPAC) will be a faculty-dominated group focused on a variety of policy issues related to shared research computing on the Morningside campus. As the use of computational tools spreads to more disciplines to create, collaborate, and disseminate knowledge, there is a commensurate rise in the costs of establishing and maintaining these resources. Shared resources have proven to leverage those available to individuals or small groups, but require careful consideration of the policies governing the shared resource and the basis of the operating model. While final authority and responsibility for such policies customarily rests with the senior administrators of the University, it is vital that the research faculty examine and recommend the policies and practices they deem best suited to accomplishing the research objectives." For more information regarding shared research computing at Columbia University, or to register for the SRCPAC ListServ, please email [email protected]. Materials from RCEC and SRCPAC Meetings RCEC Meetings FY19 Annual Report FY18 Annual Report FY17 Annual Report FY16 Annual Report FY15 Annual Report FY14 Annual Report SRCPAC Meetings Spring 2022 Minutes and Slides Fall 2021 Slides Spring 2021 Email Update (In Lieu of Meeting) Fall 2020 Email Update (In Lieu of Meeting) Spring 2020 Minutes and Slides Fall 2019 Minutes and Slides Spring 2019 Minutes and Slides Fall 2018 Minutes and Slides Spring 2018 Minutes and Slides Fall 2017 Minutes and Slides Spring 2017 Minutes and Slides Fall 2016 Email Update (In Lieu of Meeting) Spring 2016 Agenda and Minutes Appendix I (HPC Expansion) Fall, 2015 Agenda and Minutes Appendix I (Fall 2015 Yeti Operating Committee Update), Appendix II (HPC Expanion Schedule), Appendix III (Prior SRCPAC Recommendations & Actions Taken), Appendix IV (Cloud Pilot Report), Appendix V (HPC Education Resources) Spring 2015 Agenda and Minutes Appendix I (Intercampus Subcommittee Slides), Appendix II (Yeti Operating Committee Slides), Appendix III (Faculty Recruitment Guidelines & Recommendations) Fall 2014 Agenda and Minutes Appendix I (Cloud Subcommittee Slides), Appendix II (Intercampus Subcommittee Slides), Appendix III (Education Subcommittee Slides), Appendix IV (Yeti Round 2 Expansion Slides) Spring 2014 Minutes Fall 2013 Minutes (Yeti Governance) Spring 2012 Minutes Fall 2011 Agenda and Minutes Foundations for Research Computing Foundations for Research Computing provides informal training for Columbia University graduate students to develop fundamental skills for harnessing computation: core languages and libraries, software development tools, best practices, and computational problem-solving. Topics are covered from across the spectrum, from beginner to advanced. Beyond training, the Foundations program aims to create a computational community at Columbia, bringing disparate researchers together with the common thread of computation. Active SRCPAC Subcommittees Training Subcommittee Chair: Marc Spiegelman, Applied Physics & Applied Mathematics Fall 2017 Pre-Planning Meeting Minutes and Decision Report February 27th Minutes HPC Operating Committee Chair: Kyle Mandli, Applied Physics & Applied Mathematics Fall 2019 Minutes and Slides Spring 2019 Minutes and Slides (formerly Habanero Operating Committee)Spring 2017 Minutes and Slides (formerly Habanero Operating Committee) Spring 2018 Minutes and Slides (formerly Habanero Operating Committee) Fall 2017 Minutes and Slides (formerly Habanero Operating Committee) Spring 2016 Minutes and Slides (formerly Yeti Operating Committee) Fall 2015 Minutes and Slides (formerly Yeti Operating Committee) Spring 2015 Minutes and Slides (formerly Yeti Operating Committee) Shared Research Computing Facility (SRCF) The Shared Research Computing Facility (SRCF) consists of a dedicated portion of the university data center on the Morningside Campus. It is dedicated to house shared computing resources managed by CUIT, such as Columbia's centrally-managed High Performance Computing (HPC) and the Secure Data Enclave (SDE). A project to upgrade the electrical infrastructure of the data center was completed in Summer 2013*. In 2018**, cooling was expanded to increase capacity to accommodate shared computing into the foreseeable future. *The Shared Research Computing Facility project is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. **The 2018 Cooling expansion is supported by joint contributions from CUIT, the Office of the Executive Vice President for Research, Arts and Sciences, and Engineering and Applied Science. Guidance Materials SRCPAC Proposed Mandate Research Computing Services High Performance Computing Research Data Services and Amazon Web Services Columbia University Guidance on Retention of Research Data states Principal Investigators are responsible for identifying, collecting, managing, and retaining Research Data as custodian for the University. More information regarding University-wide services related to research data can be found here: https://research.columbia.edu/content/research-data-storage In addition, the University has an enterprise agreement with Amazon Web Services; Columbia researchers are encouraged to review the relevant services and explore opportunities for integrating AWS into their research programs: Account Information: https://cuit.columbia.edu/aws Cloud Computing Consulting: https://cuit.columbia.edu/cloud-research-computing-consulting Completed SRCPAC Subcommittees & Working Groups Intercampus Subcommittee Columbia Survey Working Group Cloud Subcommittee External Peer Survey Working Group Hotfoot HPC Operations Committee Manhattanville Liaison Working Group Research Storage Working Group Meetings SRCPAC meets every Fall and Spring semester for approximately 90 minutes, with select faculty, administrators, and leadership presenting updates pertaining to the University's shared research computing infrastructure. All Columbia faculty, research scientists, postdocs, students, and administrative staff are welcome to attend meetings. Meetings are scheduled and announced via the SRCPAC ListServ. To be added to this ListServ, please contact [email protected]. Information for New Faculty To provide researchers access to High Performance Computing (HPC) clusters larger than individual researchers can typically afford or wish to individually acquire and maintain, Columbia has created the Shared Research Computing Facility (SRCF) for Morningside, Lamont, and Manhattanville researchers to jointly acquire and use HPC clusters. We hope the following information will be useful as you develop your research program: If you wish to join the SRCPAC ListServ to keep informed of committee meetings and other important announcements pertaining to Columbia Shared HPC, please email [email protected]. There are three active Shared HPC clusters, including Terremoto, Habanero, and the newest system, Ginsburg. These clusters are governed by a faculty-led community, the Shared Research Computing Policy Advisory Committee (SRCPAC), and are administratively supported by full-time staff within the CUIT Research Computing Services team, providing maintenance, technical support, software installation and guidance on future computing needs. Typically each Spring, to coincide with recruiting season, faculty are polled to see if there is interest in a joint expansion round or new system purchase. A good way to ensure you are aware of upcoming events is to join the SRCPAC ListServ by emailing [email protected]. Research Computing Services (RCS) within CUIT – the entity that administratively supports the SRCF – holds online zoom office hours for HPC users from 3:00 p.m. – 5:00 p.m. on the first Monday of each month. Please RSVP here if interested. The RCS team is happy to answer questions about the SRCF, Columbia’s agreement with Google Cloud Platform, Amazon Web Services, and access to external Government-supported resources (such as XSEDE). If you have additional questions about the above broad overview, please feel free to email [email protected]. We very much hope to have you involved in governing and advancing the research computing infrastructure across Columbia University, and welcome! Publication Outcomes Research conducted on our High-Performance Computing machines has led to many peer-reviewed publications in top-tier research journals. To view citations for these publications please visit: HPC Research Publications To report new publications utilizing one or more of these machines, please email [email protected]. Recognition Text for Publications Published research emerging out of computations run on our HPCs must recognize the grants that have made this service possible. We ask that all related publications include the following acknowledgement text: We acknowledge computing resources from Columbia University's Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010. HIPAA Data The University’s shared research computing clusters are not authorized to host HIPAA-protected data. Therefore, the collection, storage, or transmission of Sensitive Data, as defined within the Columbia University Data Classification Policy, is strictly prohibited on Habanero and Yeti. Education Tier Habanero now includes an Education Tier for course instructors to use when educating students. Whereas previous shared high performance clusters offered capacity for classes deploying HPC, such use was always ranked below that of the researchers. Conversely, Habanero's current high-priority Education Tier was made possible through the generous commitments of Mary Boyce, Dean of The Fu Foundation School of Engineering and Applied Science, and David Madigan, Executive Vice President and Dean of the Faculty of Arts and Sciences. This new resource is live and ready for course adoption. To begin utilizing the Habanero Education Tier, please contact CUIT’s Research Computing Services team at [email protected]. Computational Research Training Opportunities A number of no-cost internal and external resources exist to train new and existing users in computational methodologies, high-performance computing, and data science. Please click here to view a list of the resources available to Columbia students, faculty, and staff. Contact SRCPAC Administrator [email protected] (212) 854-6841 SRCPAC Leadership Current Chair Dr. Chris Marianetti, Associate Professor, Department of Applied Physics & Applied Mathematics, 2016 - Present Past Chairs Dr. Kathryn Johnston, Professor and Chair, Department of Astronomy, 2014 - 2016 Dr. David Madigan, Former Executive Vice President and Dean, Faculty of Arts and Sciences, 2011 - 2014 Research Computing Executive Committee Dr. Jeannette Wing, Executive Vice President for Research (Chair) Dr. Mary Boyce, Provost Dr. Shih-Fu Chang, Interim Dean, The Fu Foundation School of Engineering and Applied Science Rob Cartolano, Associate Vice President for Digital Programs and Technology Services, Libraries and Information Services Dr. Robert Mawhinney, Dean of Science, Faculty of Arts and Sciences Gaspare LoDuca, Chief Information Officer and Vice President, CU Information Technology Dr. Chris Marianetti, Chair, Shared Research Computing Policy Advisory Committee Dr. Amy Hungerford, Executive Vice President and Dean, Faculty of Arts and Sciences Ann Thornton, Vice Provost and University Librarian Dr. Soulaymane Kachani, Senior Vice Provost, Office of the Provost, and Professor, Industrial Engineering and Operations Research Cliff Stein, Interim Head, Data Science Institute Victoria Hamilton, Executive Director of Research Initiatives and Development, Office of the Executive Vice President for Research (Staff) Helpful Links CUIT Research Computing Services Computational Research Training Opportunities Research Computing Services Workshops & Training Science & Engineering Library Habanero Research Publications Yeti Research Publications Hotfoot Research Publications Office of the Executive Vice President for Research313 Low Library, 116th and Broadway · New York, NY 10027 Columbia University ©2022 Columbia University Accessibility Nondiscrimination Careers Built using Columbia Sites Back to Top Close